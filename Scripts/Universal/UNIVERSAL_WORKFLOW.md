# Universal Faculty Data Collection Workflow

## Overview
This workflow uses universal scripts (2-4) that work with any college's Script 1 output. Script 1 is always custom per college, but Scripts 2-4 are standardized and reusable.

## Directory Structure
```
Scripts/
‚îú‚îÄ‚îÄ Universal/                    # Universal scripts (2-4)
‚îÇ   ‚îú‚îÄ‚îÄ 02_make_gs_links.py
‚îÇ   ‚îú‚îÄ‚îÄ 03_pull_articles.py
‚îÇ   ‚îú‚îÄ‚îÄ 04_populate_db.py
‚îÇ   ‚îú‚îÄ‚îÄ SCRIPT1_OUTPUT_FORMAT.md
‚îÇ   ‚îî‚îÄ‚îÄ UNIVERSAL_WORKFLOW.md
‚îî‚îÄ‚îÄ Drexel/
    ‚îú‚îÄ‚îÄ CS/                      # College-specific Script 1
    ‚îÇ   ‚îî‚îÄ‚îÄ 01_pull_faculty_profiles.py
    ‚îî‚îÄ‚îÄ COAS/                    # College-specific Script 1
        ‚îî‚îÄ‚îÄ 01_pull_faculty_profiles.py
```

## Workflow Steps

### Step 1: College-Specific Scraping (Custom)
**Location**: College subdirectory (e.g., `Scripts/Drexel/Engineering/`)

1. **Create college directory**:
   ```bash
   mkdir -p Scripts/Drexel/Engineering
   cd Scripts/Drexel/Engineering
   ```

2. **Write custom Script 1**:
   - Scrape faculty data from college website
   - Output `01_faculty_profiles.csv` in required format
   - See `SCRIPT1_OUTPUT_FORMAT.md` for exact specifications

3. **Run Script 1**:
   ```bash
   python 01_pull_faculty_profiles.py
   ```

4. **Verify output**:
   ```bash
   head -5 01_faculty_profiles.csv
   wc -l 01_faculty_profiles.csv
   ```

### Step 2: Google Scholar Link Discovery (Universal)
**Location**: College subdirectory

1. **Copy universal Script 2**:
   ```bash
   cp ../../Universal/02_make_gs_links.py .
   ```

2. **Run Script 2**:
   ```bash
   python 02_make_gs_links.py
   ```

3. **Verify output**:
   ```bash
   head -5 02_faculty_with_scholar_links.csv
   ```

### Step 3: Article & Headshot Extraction (Universal)
**Location**: College subdirectory

1. **Copy universal Script 3**:
   ```bash
   cp ../../Universal/03_pull_articles.py .
   ```

2. **Run Script 3**:
   ```bash
   python 03_pull_articles.py
   ```

3. **Verify output**:
   ```bash
   head -5 03_faculty_articles.csv
   head -5 03_faculty_headshots.csv
   ```

### Step 4: Database Population (Universal)
**Location**: College subdirectory

1. **Copy universal Script 4**:
   ```bash
   cp ../../Universal/04_populate_db.py .
   ```

2. **Run Script 4**:
   ```bash
   python 04_populate_db.py
   ```

## Prerequisites

### Database Setup
1. **Add college to database**:
   ```sql
   -- Add college
   INSERT INTO colleges (name, university_id) 
   VALUES ('College of Engineering', (SELECT university_id FROM universities WHERE name = 'Drexel University'));
   
   -- Add departments
   INSERT INTO departments (name, college_id) VALUES
     ('Department of Mechanical Engineering', college_id),
     ('Department of Electrical Engineering', college_id),
     -- ... add all departments
   ```

2. **Add field mappings** (if new fields needed):
   ```sql
   -- Add new fields if needed
   INSERT INTO fields_of_interest (name, description) VALUES
     ('Engineering', 'Engineering disciplines');
   
   -- Map departments to fields
   INSERT INTO department_field_mappings (department_id, field_id) VALUES
     (dept_id, field_id);
   ```

### System Requirements
- Python 3.8+
- Chrome/Chromedriver for Selenium
- Required Python packages: `requests`, `beautifulsoup4`, `selenium`, `supabase`

## Example: Adding College of Engineering

### 1. Create Directory Structure
```bash
mkdir -p Scripts/Drexel/Engineering
cd Scripts/Drexel/Engineering
```

### 2. Write Custom Script 1
```python
# 01_pull_faculty_profiles.py
# Custom scraping for Engineering faculty
# Must output 01_faculty_profiles.csv in required format
```

### 3. Run Universal Pipeline
```bash
# Step 1: Custom scraping
python 01_pull_faculty_profiles.py

# Step 2: Google Scholar links
cp ../../Universal/02_make_gs_links.py .
python 02_make_gs_links.py

# Step 3: Articles & headshots
cp ../../Universal/03_pull_articles.py .
python 03_pull_articles.py

# Step 4: Database population
cp ../../Universal/04_populate_db.py .
python 04_populate_db.py
```

## File Dependencies

### Input Files (Generated by Previous Steps)
- `01_faculty_profiles.csv` ‚Üê Script 1 output
- `02_faculty_with_scholar_links.csv` ‚Üê Script 2 output
- `03_faculty_articles.csv` ‚Üê Script 3 output
- `03_faculty_headshots.csv` ‚Üê Script 3 output

### Output Files (Generated by Each Step)
- Script 1: `01_faculty_profiles.csv`
- Script 2: `02_faculty_with_scholar_links.csv`
- Script 3: `03_faculty_articles.csv`, `03_faculty_headshots.csv`
- Script 4: Database records

## Troubleshooting

### Common Issues

#### Script 1 Issues
- **Wrong CSV format**: Check `SCRIPT1_OUTPUT_FORMAT.md`
- **Missing departments**: Verify department names match database
- **Encoding problems**: Ensure UTF-8 encoding

#### Script 2 Issues
- **No Google Scholar links found**: Check faculty names are properly formatted
- **Rate limiting**: Script includes delays, but may need adjustment

#### Script 3 Issues
- **Article extraction fails**: Check Google Scholar links are valid
- **URL formatting**: Script handles URL cleaning automatically

#### Script 4 Issues
- **Database errors**: Verify department names exist in database
- **Missing dependencies**: Install required Python packages

### Validation Commands
```bash
# Check Script 1 output
python3 -c "import csv; reader = csv.DictReader(open('01_faculty_profiles.csv')); print('Headers:', reader.fieldnames); print('Count:', sum(1 for row in reader))"

# Check Script 2 output
python3 -c "import csv; reader = csv.DictReader(open('02_faculty_with_scholar_links.csv')); found = sum(1 for row in reader if row['Found'] == 'Yes'); print(f'Found {found} Google Scholar profiles')"

# Check Script 3 output
wc -l 03_faculty_articles.csv
wc -l 03_faculty_headshots.csv
```

## Best Practices

### Script 1 Development
1. **Start with a small test** - Scrape 2-3 faculty first
2. **Validate output format** - Use validation commands above
3. **Handle edge cases** - Missing emails, multiple departments, etc.
4. **Test thoroughly** - Ensure all faculty are captured

### Universal Scripts Usage
1. **Always copy fresh scripts** - Don't modify universal scripts
2. **Check prerequisites** - Ensure database setup is complete
3. **Monitor progress** - Scripts provide detailed output
4. **Validate results** - Check output files after each step

### Database Management
1. **Add departments first** - Before running Script 4
2. **Map to fields** - Ensure department-field mappings exist
3. **Test queries** - Verify data is accessible via API
4. **Backup data** - Before major changes

## Scaling to Multiple Universities

### For Different Universities
1. **Create university directory**: `Scripts/UniversityName/`
2. **Add university to database**: Insert into `universities` table
3. **Add colleges**: Insert into `colleges` table
4. **Follow same workflow**: Script 1 custom, Scripts 2-4 universal

### Example Structure for Multiple Universities
```
Scripts/
‚îú‚îÄ‚îÄ Universal/                    # Universal scripts
‚îú‚îÄ‚îÄ Drexel/                      # Drexel University
‚îÇ   ‚îú‚îÄ‚îÄ CS/
‚îÇ   ‚îú‚îÄ‚îÄ COAS/
‚îÇ   ‚îî‚îÄ‚îÄ Engineering/
‚îî‚îÄ‚îÄ Penn/                        # University of Pennsylvania
    ‚îú‚îÄ‚îÄ Engineering/
    ‚îî‚îÄ‚îÄ Medicine/
```

This universal approach makes it easy to scale to any number of colleges and universities! üéØ 